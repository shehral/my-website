// ─────────────────────────────────────────────
// Library — Resource Data Layer
// Gwern-inspired personal knowledge index
// ─────────────────────────────────────────────

export type ResourceType = "paper" | "post" | "video" | "tool" | "course" | "book" | "community"

export type ResourceCategory =
  | "interpretability"
  | "agents"
  | "safety"
  | "foundations"
  | "infrastructure"
  | "culture"
  | "tools"

export type ImportanceLevel = 1 | 2 | 3 // ★ interesting, ★★ solid, ★★★ foundational

export interface LibraryResource {
  title: string
  href: string
  annotation: string
  type: ResourceType
  importance: ImportanceLevel
  category: ResourceCategory
  tags: string[]
}

// ─── Interpretability ──────────────────────────

const interpretability: LibraryResource[] = [
  // Papers
  {
    title: "Open Problems in Mechanistic Interpretability",
    href: "https://arxiv.org/abs/2501.16496",
    annotation: "Nanda et al. comprehensive roadmap of unsolved problems. Start here for research directions.",
    type: "paper",
    importance: 3,
    category: "interpretability",
    tags: ["survey", "mech-interp", "research-agenda"],
  },
  {
    title: "Emergent Misalignment Paper and Follow-Up Work",
    href: "https://www.emergent-misalignment.com/",
    annotation: "Fine-tuning on insecure code produces misaligned behavior across unrelated tasks. Startling transfer.",
    type: "paper",
    importance: 3,
    category: "interpretability",
    tags: ["alignment", "fine-tuning", "emergent"],
  },
  {
    title: "Google DeepMind — Agentic Interpretability",
    href: "https://arxiv.org/pdf/2506.12152",
    annotation: "Using AI agents to interpret other AI systems. Meta-interpretability at scale.",
    type: "paper",
    importance: 2,
    category: "interpretability",
    tags: ["agents", "deepmind", "automation"],
  },
  {
    title: "Circuit Tracing: Revealing Computational Graphs in Language Models",
    href: "https://transformer-circuits.pub/2025/attribution-graphs/methods.html",
    annotation: "Anthropic's methods paper for attribution graphs. The technical foundation for the 'biology' companion paper.",
    type: "paper",
    importance: 3,
    category: "interpretability",
    tags: ["anthropic", "circuits", "attribution"],
  },
  {
    title: "Improving LLM Fairness via Interpretability",
    href: "https://arxiv.org/pdf/2506.10922",
    annotation: "Using mech-interp techniques to find and fix bias in language models.",
    type: "paper",
    importance: 2,
    category: "interpretability",
    tags: ["fairness", "bias", "applied"],
  },
  {
    title: "OpenAI — Persona Features Control Emergent Misalignment",
    href: "https://openai.com/index/emergent-misalignment/",
    annotation: "Specific features in SAEs correspond to 'personas' that drive misaligned behavior.",
    type: "paper",
    importance: 3,
    category: "interpretability",
    tags: ["openai", "SAE", "misalignment"],
  },
  {
    title: "Understanding Reasoning in Thinking Models via Steering Vectors",
    href: "https://arxiv.org/abs/2506.18167",
    annotation: "Steering vectors applied to chain-of-thought models. Can you nudge reasoning without breaking it?",
    type: "paper",
    importance: 2,
    category: "interpretability",
    tags: ["steering", "CoT", "reasoning"],
  },
  {
    title: "Automatically Interpreting Millions of Features in LLMs",
    href: "https://arxiv.org/html/2410.13928v2",
    annotation: "OpenAI's automated neuron labeling at scale. GPT-4 explaining GPT-2's neurons.",
    type: "paper",
    importance: 2,
    category: "interpretability",
    tags: ["automation", "SAE", "openai"],
  },
  {
    title: "Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability",
    href: "https://arxiv.org/abs/2301.04709",
    annotation: "Formal framework connecting high-level causal models to neural network computation.",
    type: "paper",
    importance: 3,
    category: "interpretability",
    tags: ["theory", "causal", "foundations"],
  },
  {
    title: "Is Causal Abstraction Enough for Mechanistic Interpretability?",
    href: "https://arxiv.org/abs/2507.08802",
    annotation: "The rebuttal. Argues causal abstraction misses important aspects of neural computation.",
    type: "paper",
    importance: 2,
    category: "interpretability",
    tags: ["theory", "causal", "critique"],
  },
  {
    title: "Chain of Thought Monitorability — A New and Fragile Opportunity for AI Safety",
    href: "https://arxiv.org/abs/2507.11473",
    annotation: "CoT as a safety tool: models that reason out loud are easier to audit. But is the reasoning faithful?",
    type: "paper",
    importance: 2,
    category: "interpretability",
    tags: ["CoT", "monitoring", "safety"],
  },
  {
    title: "Understanding and Steering Llama 3 with SAEs",
    href: "https://www.goodfire.ai/papers/understanding-and-steering-llama-3",
    annotation: "Practical SAE application to Meta's Llama 3. Good reference for hands-on interp work.",
    type: "paper",
    importance: 2,
    category: "interpretability",
    tags: ["SAE", "llama", "practical"],
  },
  {
    title: "Auditing LLMs for Hidden Objectives",
    href: "https://www.anthropic.com/research/auditing-hidden-objectives",
    annotation: "Methods for detecting when models pursue goals they weren't trained on.",
    type: "paper",
    importance: 2,
    category: "interpretability",
    tags: ["auditing", "safety", "deception"],
  },
  {
    title: "A Survey of Mech Interp for Multimodal Foundational Models",
    href: "https://arxiv.org/abs/2502.17516",
    annotation: "Extending interpretability beyond text — vision transformers, multimodal models.",
    type: "paper",
    importance: 2,
    category: "interpretability",
    tags: ["multimodal", "survey", "vision"],
  },
  {
    title: "Subliminal Learning: Language Models Transmit Behavioral Traits via Hidden Signals",
    href: "https://arxiv.org/abs/2507.14805",
    annotation: "Models can encode behavioral tendencies in ways humans can't detect. Eerie.",
    type: "paper",
    importance: 2,
    category: "interpretability",
    tags: ["steganography", "behavior", "hidden"],
  },
  {
    title: "A Primer on the Inner Workings of Transformer-based LMs",
    href: "https://arxiv.org/abs/2405.00208",
    annotation: "If you read one 'how transformers work' paper, make it this one. Clear and thorough.",
    type: "paper",
    importance: 3,
    category: "interpretability",
    tags: ["primer", "transformer", "foundations"],
  },
  {
    title: "Mathematical Framework for Transformer Circuits",
    href: "https://transformer-circuits.pub/2021/framework/index.html",
    annotation: "Elhage et al.'s foundational work. The mathematical lens for all circuit-level analysis.",
    type: "paper",
    importance: 3,
    category: "interpretability",
    tags: ["theory", "circuits", "math"],
  },
  {
    title: "Inference Time Decomposition of Activations (ITDA)",
    href: "https://arxiv.org/abs/2505.17769",
    annotation: "Decomposing model activations at inference time. New tool for real-time interpretability.",
    type: "paper",
    importance: 2,
    category: "interpretability",
    tags: ["activations", "method", "inference"],
  },
  {
    title: "Thought Anchors — Which Reasoning Steps Matter",
    href: "https://www.thought-anchors.com/",
    annotation: "Not all reasoning steps are equal. This finds the 'anchors' that actually determine the output.",
    type: "paper",
    importance: 2,
    category: "interpretability",
    tags: ["reasoning", "CoT", "analysis"],
  },
  {
    title: "Cost-effective Constitutional Classifiers via Representation Reuse",
    href: "https://alignment.anthropic.com/2025/cheap-monitors/",
    annotation: "Reusing internal representations to build cheaper safety classifiers. Efficiency meets safety.",
    type: "paper",
    importance: 2,
    category: "interpretability",
    tags: ["classifiers", "efficiency", "safety"],
  },
  {
    title: "Detecting Strategic Deception Using Linear Probes",
    href: "https://www.apolloresearch.ai/research/deception-probes",
    annotation: "Can simple linear probes catch a model that's actively trying to deceive? Surprisingly: often yes.",
    type: "paper",
    importance: 2,
    category: "interpretability",
    tags: ["deception", "probes", "safety"],
  },
  {
    title: "Multimodal Automated Interpretability Agents",
    href: "https://arxiv.org/abs/2404.14394",
    annotation: "OpenAI's vision for AI systems that can interpret other AI systems across modalities.",
    type: "paper",
    importance: 2,
    category: "interpretability",
    tags: ["automation", "multimodal", "openai"],
  },
  {
    title: "On the Biology of a Large Language Model",
    href: "https://transformer-circuits.pub/2025/attribution-graphs/biology.html",
    annotation: "Anthropic treats Claude as a biological specimen and dissects it. Beautiful writing meets rigorous science.",
    type: "paper",
    importance: 3,
    category: "interpretability",
    tags: ["anthropic", "biology", "circuits"],
  },
  // Posts
  {
    title: "The Urgency of Interpretability",
    href: "https://www.darioamodei.com/post/the-urgency-of-interpretability",
    annotation: "Dario Amodei's call to arms. Why interp matters more than ever as models get powerful.",
    type: "post",
    importance: 3,
    category: "interpretability",
    tags: ["anthropic", "leadership", "urgency"],
  },
  {
    title: "How to Become a Mechanistic Interpretability Researcher",
    href: "https://www.alignmentforum.org/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher",
    annotation: "Neel Nanda's career guide for aspiring interp researchers. Practical and honest.",
    type: "post",
    importance: 3,
    category: "interpretability",
    tags: ["career", "neel-nanda", "getting-started"],
  },
  {
    title: "Concrete Steps to Get Started in Mech Interp",
    href: "https://www.neelnanda.io/mechanistic-interpretability/getting-started",
    annotation: "The practical companion to the career guide. Step-by-step onboarding for new researchers.",
    type: "post",
    importance: 2,
    category: "interpretability",
    tags: ["getting-started", "practical", "neel-nanda"],
  },
  {
    title: "200 Concrete Open Problems in Mech Interp",
    href: "https://www.alignmentforum.org/posts/LbrPTJ4fmABEdEnLf/200-concrete-open-problems-in-mechanistic-interpretability",
    annotation: "The original problem list. Many still open. Great for finding thesis/project topics.",
    type: "post",
    importance: 2,
    category: "interpretability",
    tags: ["problems", "research", "neel-nanda"],
  },
  {
    title: "45+ Project Ideas for Mech Interp",
    href: "https://www.alignmentforum.org/posts/KfkpgXdgRheSRWDy8/a-list-of-45-mech-interp-project-ideas-from-apollo-research",
    annotation: "More actionable than the 200 problems list. Good for weekend or hackathon projects.",
    type: "post",
    importance: 2,
    category: "interpretability",
    tags: ["projects", "practical", "neel-nanda"],
  },
  {
    title: "An Intuitive Explanation of Sparse Autoencoders",
    href: "https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html",
    annotation: "The gentlest on-ramp to understanding SAEs. Visual, intuitive, no heavy math.",
    type: "post",
    importance: 3,
    category: "interpretability",
    tags: ["SAE", "explainer", "beginner"],
  },
  {
    title: "Neel Nanda — Mech Interp Glossary",
    href: "https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J",
    annotation: "Keep this tab open. You'll reference it constantly.",
    type: "post",
    importance: 2,
    category: "interpretability",
    tags: ["reference", "glossary", "neel-nanda"],
  },
  {
    title: "Opinionated List of Mech Interp Papers",
    href: "https://www.alignmentforum.org/posts/NfFST5Mio7BCAQHPA/an-extremely-opinionated-annotated-list-of-my-favourite-1",
    annotation: "Neel's curated reading list with commentary on each paper's importance.",
    type: "post",
    importance: 2,
    category: "interpretability",
    tags: ["reading-list", "papers", "neel-nanda"],
  },
  {
    title: "A Longlist of Theories of Impact for Interpretability",
    href: "https://www.lesswrong.com/posts/uKYuLFByxLBbGRc4w/a-longlist-of-theories-of-impact-for-interpretability",
    annotation: "Every argument for why interp matters, catalogued. The optimist's reference.",
    type: "post",
    importance: 2,
    category: "interpretability",
    tags: ["impact", "theory-of-change"],
  },
  {
    title: "Against Almost Every Theory of Impact for Interpretability",
    href: "https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1",
    annotation: "The steel-man counterargument. Read both this and the longlist for a balanced view.",
    type: "post",
    importance: 2,
    category: "interpretability",
    tags: ["impact", "critique", "debate"],
  },
  {
    title: "The Building Blocks of Interpretability",
    href: "https://distill.pub/2018/building-blocks/",
    annotation: "Distill classic. Feature visualization, attribution, and the foundational ideas of interp.",
    type: "post",
    importance: 3,
    category: "interpretability",
    tags: ["distill", "foundations", "visualization"],
  },
  {
    title: "A Stylised Recent History of Mech Interp",
    href: "https://docs.google.com/document/d/1MpTWonK0LACVHecEl8OaMMd1KKPxePo-HXD8eBc9RVg/edit?usp=sharing",
    annotation: "How the field evolved from Olah's circuits to SAEs and attribution graphs.",
    type: "post",
    importance: 2,
    category: "interpretability",
    tags: ["history", "overview"],
  },
  {
    title: "Results from Interpretability Hackathon",
    href: "https://www.lesswrong.com/posts/9L6sCxDvQ3bMmQ9hs/results-from-the-interpretability-hackathon",
    annotation: "What actually comes out of a weekend interp hackathon. Surprisingly strong results.",
    type: "post",
    importance: 1,
    category: "interpretability",
    tags: ["hackathon", "community", "results"],
  },
  {
    title: "Simple Probes Can Catch Sleeper Agents",
    href: "https://www.anthropic.com/research/probes-catch-sleeper-agents",
    annotation: "Linear probes detecting backdoor behavior. Simple beats complex in safety tooling.",
    type: "post",
    importance: 2,
    category: "interpretability",
    tags: ["probes", "sleeper-agents", "safety"],
  },
  {
    title: "Painting with Concepts Using Diffusion Model Latents",
    href: "https://www.goodfire.ai/blog/painting-with-concepts",
    annotation: "Interpretability meets generative art. Manipulating diffusion model features to 'paint.'",
    type: "post",
    importance: 1,
    category: "interpretability",
    tags: ["diffusion", "creative", "visualization"],
  },
  {
    title: "Goodfire — On Optimism for Interpretability",
    href: "https://www.goodfire.ai/blog/on-optimism-for-interpretability",
    annotation: "Goodfire's case that commercial interp is viable. The startup perspective.",
    type: "post",
    importance: 1,
    category: "interpretability",
    tags: ["industry", "startup", "optimism"],
  },
  // Videos
  {
    title: "David Bau at UC Berkeley",
    href: "https://www.youtube.com/watch?v=8QPVKpzyZdY",
    annotation: "Bau's latest on representation engineering and model editing. Deep technical talk.",
    type: "video",
    importance: 2,
    category: "interpretability",
    tags: ["lecture", "david-bau", "representation"],
  },
  {
    title: "David Bau at Vienna Alignment Workshop",
    href: "https://www.youtube.com/watch?v=AIfmSx250kM",
    annotation: "Focused on alignment implications of interpretability work. More philosophical.",
    type: "video",
    importance: 2,
    category: "interpretability",
    tags: ["lecture", "david-bau", "alignment"],
  },
  {
    title: "David Bau — Direct Model Editing and Mech Interp",
    href: "https://www.youtube.com/watch?v=I1ELSZNFeHc",
    annotation: "The ROME/MEMIT talk. Editing factual knowledge directly in model weights.",
    type: "video",
    importance: 2,
    category: "interpretability",
    tags: ["lecture", "david-bau", "editing"],
  },
  {
    title: "David Bau — Talk at MIT CSAIL",
    href: "https://www.youtube.com/watch?v=2L3wRwSseqc",
    annotation: "Comprehensive overview of Bau's lab's work. Best single introduction to his research program.",
    type: "video",
    importance: 2,
    category: "interpretability",
    tags: ["lecture", "david-bau", "overview"],
  },
  {
    title: "Goodfire CEO Eric Ho — Mapping the Mind of a Neural Net (Sequoia Training Data)",
    href: "https://www.youtube.com/watch?v=lDTEFogB_Us",
    annotation: "Building a company on interpretability. Business case for understanding AI internals.",
    type: "video",
    importance: 1,
    category: "interpretability",
    tags: ["podcast", "startup", "industry"],
  },
  {
    title: "Anthropic — Scaling Interpretability",
    href: "https://www.youtube.com/watch?v=sQar5NNGbw4",
    annotation: "Anthropic's talk on scaling SAE analysis to production models. The engineering challenges.",
    type: "video",
    importance: 2,
    category: "interpretability",
    tags: ["anthropic", "scaling", "engineering"],
  },
  {
    title: "Anthropic — Interpretability: Understanding How AI Models Think",
    href: "https://www.youtube.com/watch?v=fGKNUvivvnc",
    annotation: "Anthropic researchers discuss peering inside Claude's mind. Good for sharing with non-experts.",
    type: "video",
    importance: 2,
    category: "interpretability",
    tags: ["anthropic", "overview", "accessible"],
  },
  {
    title: "Rational Animations",
    href: "https://www.youtube.com/@RationalAnimations",
    annotation: "Animated explainers on AI safety and rationality. High production value, accurate content.",
    type: "video",
    importance: 1,
    category: "interpretability",
    tags: ["animation", "explainer", "accessible"],
  },
  {
    title: "Neel Nanda on 80,000 Hours Podcast",
    href: "https://www.youtube.com/watch?v=5FdO1MEumbI",
    annotation: "3+ hours of Neel on mech interp's promise, limits, and career advice. The definitive podcast episode.",
    type: "video",
    importance: 3,
    category: "interpretability",
    tags: ["podcast", "neel-nanda", "career"],
  },
  // Tools / Other
  {
    title: "Gemma Scope",
    href: "https://www.neuronpedia.org/gemma-scope#main",
    annotation: "Interactive SAE feature viewer for Google's Gemma models. Explore features in your browser.",
    type: "tool",
    importance: 2,
    category: "interpretability",
    tags: ["SAE", "visualization", "google"],
  },
  {
    title: "Demo: Gemma Scope",
    href: "https://www.youtube.com/watch?v=X1gDXDQu_wU",
    annotation: "Walkthrough of Gemma Scope's interface. See SAE features in action before diving in yourself.",
    type: "video",
    importance: 1,
    category: "interpretability",
    tags: ["SAE", "demo", "google"],
  },
  {
    title: "Discord Server for Mech Interp",
    href: "https://www.lesswrong.com/posts/gGysHedD8aHKueb35/creating-a-discord-server-for-mechanistic-interpretability",
    annotation: "The community hub. Paper discussions, job postings, research collaborations.",
    type: "community",
    importance: 2,
    category: "interpretability",
    tags: ["community", "discord", "networking"],
  },
  {
    title: "AI Safety Readings — Harvard",
    href: "https://boazbk.github.io/mltheoryseminar/",
    annotation: "Harvard's curated AI safety curriculum. Rigorous academic framing.",
    type: "course",
    importance: 2,
    category: "interpretability",
    tags: ["curriculum", "harvard", "safety"],
  },
]

// ─── Safety & Alignment ────────────────────────

const safety: LibraryResource[] = [
  {
    title: "Understanding Strategic Deception and Deceptive Alignment",
    href: "https://www.lesswrong.com/posts/GCQFkp74iikb6Fq6m/ai-s-hidden-game-understanding-strategic-deception-in-ai-and",
    annotation: "Apollo Research's framework for thinking about AI deception. Essential safety reading.",
    type: "post",
    importance: 3,
    category: "safety",
    tags: ["deception", "strategy", "alignment"],
  },
  {
    title: "Situational Awareness — The Decade Ahead",
    href: "https://situational-awareness.ai/",
    annotation: "Leopold Aschenbrenner's 165-page treatise on where AI is heading. Controversial, influential, thorough.",
    type: "paper",
    importance: 3,
    category: "safety",
    tags: ["forecasting", "geopolitics", "scaling"],
  },
  {
    title: "How to Form Inside Views about AI Safety",
    href: "https://www.neelnanda.io/blog/47-inside-views",
    annotation: "Meta-guide on developing your own safety worldview rather than deferring to experts.",
    type: "post",
    importance: 2,
    category: "safety",
    tags: ["epistemics", "worldview", "methodology"],
  },
  {
    title: "Good Research Takes Are Not Sufficient for Good Strategic Takes",
    href: "https://www.lesswrong.com/posts/P5zWiPF5cPJZSkiAK/good-research-takes-are-not-sufficient-for-good-strategic",
    annotation: "Being right about the science doesn't mean you'll be right about the policy. Important distinction.",
    type: "post",
    importance: 2,
    category: "safety",
    tags: ["strategy", "policy", "epistemics"],
  },
  {
    title: "An Approach to Technical AGI Safety and Security — Google DeepMind",
    href: "https://arxiv.org/pdf/2504.01849",
    annotation: "DeepMind's most detailed safety framework. Monitoring, containment, alignment in one document.",
    type: "paper",
    importance: 3,
    category: "safety",
    tags: ["deepmind", "framework", "comprehensive"],
  },
  {
    title: "Levels of AGI for Operationalizing Progress",
    href: "https://arxiv.org/abs/2311.02462",
    annotation: "DeepMind's AGI taxonomy. Performance × autonomy matrix. Useful shared vocabulary.",
    type: "paper",
    importance: 2,
    category: "safety",
    tags: ["AGI", "taxonomy", "deepmind"],
  },
  {
    title: "Towards Understanding Sycophancy in LLMs",
    href: "https://arxiv.org/abs/2310.13548",
    annotation: "Why RLHF makes models agree with you even when you're wrong. The sycophancy problem.",
    type: "paper",
    importance: 2,
    category: "safety",
    tags: ["RLHF", "sycophancy", "alignment"],
  },
  {
    title: "The Strawberry Problem",
    href: "https://arxiv.org/abs/2505.14172",
    annotation: "Can you get an AI to count the 'r's in 'strawberry'? Deceptively simple alignment test case.",
    type: "post",
    importance: 1,
    category: "safety",
    tags: ["alignment", "evaluation", "thought-experiment"],
  },
  {
    title: "Harvard CS 2881r — AI Alignment and Safety (Boaz Barak)",
    href: "https://boazbk.github.io/mltheoryseminar/",
    annotation: "Harvard's graduate AI safety course. Lectures, readings, and problem sets available online.",
    type: "course",
    importance: 2,
    category: "safety",
    tags: ["course", "harvard", "academic"],
  },
  {
    title: "AI-Induced Psychosis — A Shallow Investigation",
    href: "https://www.lesswrong.com/posts/iGF7YcnQkEbwvYLPA/ai-induced-psychosis-a-shallow-investigation",
    annotation: "Early documentation of psychological effects of intense AI interaction. Worth monitoring.",
    type: "post",
    importance: 1,
    category: "safety",
    tags: ["psychology", "risk", "human-AI"],
  },
]

// ─── Agents ────────────────────────────────────

const agents: LibraryResource[] = [
  {
    title: "Advances and Challenges in Foundation Agents",
    href: "https://arxiv.org/abs/2504.01990",
    annotation: "200+ page survey of the entire agentic AI landscape. The definitive reference for 2025.",
    type: "paper",
    importance: 3,
    category: "agents",
    tags: ["survey", "comprehensive", "2025"],
  },
  {
    title: "A Survey of Self-Evolving Agents",
    href: "https://arxiv.org/abs/2508.07407",
    annotation: "Agents that modify their own prompts, tools, and memory. The next frontier of agent development.",
    type: "paper",
    importance: 2,
    category: "agents",
    tags: ["self-improvement", "evolution", "survey"],
  },
  {
    title: "Small Language Models are the Future of Agentic AI",
    href: "https://research.nvidia.com/labs/lpr/slm-agents/",
    annotation: "Specialized 1-7B models crush agentic benchmarks. You don't need frontier models for tool use.",
    type: "paper",
    importance: 2,
    category: "agents",
    tags: ["small-models", "efficiency", "practical"],
  },
  {
    title: "A World Model for Social Simulation",
    href: "https://arxiv.org/abs/2504.10157",
    annotation: "Generative Agents sequel. LLM-powered societies with persistent memory and emergent dynamics.",
    type: "paper",
    importance: 2,
    category: "agents",
    tags: ["simulation", "social", "emergent"],
  },
  {
    title: "LLM Agents as Research Assistants",
    href: "https://mail.bycloud.ai/p/agent-laboratory-using-llm-agents-as-research-assistants",
    annotation: "Honest assessment: great at lit review, mediocre at ideation, terrible at knowing when wrong.",
    type: "post",
    importance: 2,
    category: "agents",
    tags: ["research", "practical", "assessment"],
  },
  {
    title: "Anthropic — How We Built Our Multi-Agent Research System",
    href: "https://www.anthropic.com/engineering/multi-agent-research-system",
    annotation: "Orchestrator pattern, sandboxed execution, and honest post-mortem of failures.",
    type: "post",
    importance: 3,
    category: "agents",
    tags: ["anthropic", "architecture", "engineering"],
  },
  {
    title: "Google DeepMind — AlphaEvolve",
    href: "https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/",
    annotation: "AI writing better algorithms via evolution. Discovered new matrix multiplication methods.",
    type: "post",
    importance: 2,
    category: "agents",
    tags: ["deepmind", "evolution", "algorithms"],
  },
  {
    title: "Context is the Next Frontier",
    href: "https://www.youtube.com/watch?v=wJyl6kBCwmY",
    annotation: "The bottleneck isn't parameters — it's context architecture. Memory > scale.",
    type: "post",
    importance: 2,
    category: "agents",
    tags: ["context", "memory", "architecture"],
  },
]

// ─── Foundations ────────────────────────────────

const foundations: LibraryResource[] = [
  {
    title: "Reinforcement Learning — An Overview",
    href: "https://arxiv.org/abs/2412.05265",
    annotation: "Modern RL survey covering everything from bandits to RLHF. Comprehensive reference.",
    type: "paper",
    importance: 2,
    category: "foundations",
    tags: ["RL", "survey", "comprehensive"],
  },
  {
    title: "Fundamental Limitations on Subquadratic Alternatives to Transformers",
    href: "https://arxiv.org/abs/2410.04271",
    annotation: "Theoretical proof that you can't escape quadratic attention without losing expressivity. Sobering.",
    type: "paper",
    importance: 2,
    category: "foundations",
    tags: ["theory", "attention", "complexity"],
  },
  {
    title: "Titans — Learning to Memorize at Test Time",
    href: "https://arxiv.org/abs/2501.00663",
    annotation: "New architecture with learnable memory modules. Potential transformer alternative.",
    type: "paper",
    importance: 2,
    category: "foundations",
    tags: ["architecture", "memory", "innovation"],
  },
  {
    title: "Apple — The Illusion of Thinking",
    href: "https://machinelearning.apple.com/research/illusion-of-thinking",
    annotation: "Apple's controversial claim that LLMs don't really reason. Sparked months of debate.",
    type: "paper",
    importance: 2,
    category: "foundations",
    tags: ["reasoning", "apple", "debate"],
  },
  {
    title: "Order Matters in Hallucination",
    href: "https://arxiv.org/abs/2408.05093",
    annotation: "The order of few-shot examples dramatically affects hallucination rates. Simple insight, big impact.",
    type: "paper",
    importance: 1,
    category: "foundations",
    tags: ["hallucination", "prompting", "practical"],
  },
  {
    title: "A Survey on Efficient Architectures for LLMs",
    href: "https://arxiv.org/abs/2508.09834",
    annotation: "Everything being tried to make LLMs faster and cheaper. MoE, distillation, quantization, and more.",
    type: "paper",
    importance: 2,
    category: "foundations",
    tags: ["efficiency", "survey", "architecture"],
  },
  {
    title: "Sleep-time Compute",
    href: "https://arxiv.org/abs/2504.13171",
    annotation: "Pre-computation between queries. Like dreaming for AI. Creative approach to inference costs.",
    type: "paper",
    importance: 2,
    category: "foundations",
    tags: ["inference", "efficiency", "innovative"],
  },
  {
    title: "Why Language Models Hallucinate — OpenAI",
    href: "https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf",
    annotation: "OpenAI's internal analysis of hallucination sources. Training data, RLHF, and decoding all contribute.",
    type: "paper",
    importance: 2,
    category: "foundations",
    tags: ["hallucination", "openai", "analysis"],
  },
  {
    title: "Implicit Reasoning in LLMs — A Survey",
    href: "https://arxiv.org/abs/2509.02350",
    annotation: "Do models reason even without chain-of-thought? Survey of evidence and mechanisms.",
    type: "paper",
    importance: 2,
    category: "foundations",
    tags: ["reasoning", "implicit", "survey"],
  },
  {
    title: "How Much Do Language Models Memorize?",
    href: "https://arxiv.org/abs/2505.24832",
    annotation: "Quantifying memorization vs. generalization. The line is blurrier than you'd hope.",
    type: "paper",
    importance: 2,
    category: "foundations",
    tags: ["memorization", "generalization", "analysis"],
  },
  {
    title: "OpenAI — Key Papers in Deep RL",
    href: "https://spinningup.openai.com/en/latest/spinningup/keypapers.html",
    annotation: "OpenAI's curated reading list for deep RL. Annotated and organized by topic.",
    type: "post",
    importance: 3,
    category: "foundations",
    tags: ["RL", "reading-list", "openai"],
  },
  {
    title: "Ilya Sutskever's Top 30 Papers",
    href: "https://aman.ai/primers/ai/top-30-papers/",
    annotation: "The reading list Ilya reportedly gives new researchers. Foundational and opinionated.",
    type: "post",
    importance: 3,
    category: "foundations",
    tags: ["reading-list", "foundational", "classic"],
  },
  {
    title: "The Roadmap of Mathematics for Machine Learning",
    href: "https://thepalindrome.org/p/the-roadmap-of-mathematics-for-machine-learning",
    annotation: "Everything from linear algebra to measure theory. Know what you need and what you can skip.",
    type: "post",
    importance: 2,
    category: "foundations",
    tags: ["math", "curriculum", "learning"],
  },
  {
    title: "3Blue1Brown — Neural Networks",
    href: "https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi",
    annotation: "The best visual explanation of neural networks ever made. Grant Sanderson at his finest.",
    type: "video",
    importance: 3,
    category: "foundations",
    tags: ["visual", "beginner", "3blue1brown"],
  },
  {
    title: "3Blue1Brown — Podcast with Lex Fridman",
    href: "https://www.youtube.com/watch?v=U_lKUK2MCsg",
    annotation: "Grant on mathematical thinking, teaching, and the philosophy of understanding.",
    type: "video",
    importance: 1,
    category: "foundations",
    tags: ["podcast", "math", "philosophy"],
  },
  {
    title: "Why Deep Learning Works Unreasonably Well — Welch Labs",
    href: "https://www.youtube.com/watch?v=qx7hirqgfuU",
    annotation: "Visual series on the mathematical reasons neural networks generalize. Excellent production.",
    type: "video",
    importance: 2,
    category: "foundations",
    tags: ["visual", "theory", "welch-labs"],
  },
  {
    title: "Stanford Applied Physics 293 — Explainable AI",
    href: "https://docs.google.com/document/d/1WjvCrQhljXQh4zOblmlXFlC-wWTWfc6f9XZKGRN_6eU/edit?tab=t.0",
    annotation: "Stanford course bridging physics intuition with AI interpretability. Unique perspective.",
    type: "course",
    importance: 2,
    category: "foundations",
    tags: ["course", "stanford", "physics"],
  },
]

// ─── Infrastructure ────────────────────────────

const infrastructure: LibraryResource[] = [
  {
    title: "DeepSeek V3 — Hardware for AI Architectures",
    href: "https://arxiv.org/abs/2505.09343",
    annotation: "How DeepSeek co-designed model architecture with hardware constraints. The future of efficient training.",
    type: "paper",
    importance: 2,
    category: "infrastructure",
    tags: ["hardware", "deepseek", "training"],
  },
  {
    title: "NVIDIA: Groot N1 — Open Model for Humanoid Robots",
    href: "https://research.nvidia.com/publication/2025-03_nvidia-isaac-gr00t-n1-open-foundation-model-humanoid-robots",
    annotation: "Open foundation model for robot control. Jensen's bet on embodied AI.",
    type: "paper",
    importance: 1,
    category: "infrastructure",
    tags: ["robotics", "nvidia", "embodied"],
  },
  {
    title: "Universal Deep Research — NVIDIA",
    href: "https://research.nvidia.com/labs/lpr/udr/",
    annotation: "NVIDIA's framework for AI-powered scientific research. GPU-accelerated discovery.",
    type: "paper",
    importance: 1,
    category: "infrastructure",
    tags: ["nvidia", "research", "tools"],
  },
  {
    title: "A Survey on Autonomous Scientific Discovery",
    href: "https://arxiv.org/abs/2508.14111",
    annotation: "Can AI do science autonomously? Survey of AI scientist systems, benchmarks, and limitations.",
    type: "paper",
    importance: 2,
    category: "infrastructure",
    tags: ["science", "automation", "survey"],
  },
  {
    title: "Inside NVIDIA GPUs — Anatomy of High-Performance Matmul Kernels",
    href: "https://www.aleksagordic.com/blog/matmul",
    annotation: "From CUDA basics to optimized matrix multiplication. Essential systems knowledge for ML engineers.",
    type: "post",
    importance: 2,
    category: "infrastructure",
    tags: ["CUDA", "GPU", "systems"],
  },
  {
    title: "Gartner Hype Cycle for AI",
    href: "https://www.gartner.com/en/newsroom/press-releases/2025-08-05-gartner-hype-cycle-identifies-top-ai-innovations-in-2025",
    annotation: "Where every AI technology sits on the hype curve. Useful calibration tool.",
    type: "post",
    importance: 1,
    category: "infrastructure",
    tags: ["industry", "analysis", "trends"],
  },
]

// ─── Culture & Perspectives ────────────────────

const culture: LibraryResource[] = [
  {
    title: "You and Your Research — Richard Hamming",
    href: "https://www.cs.virginia.edu/~robins/YouAndYourResearch.html",
    annotation: "Hamming's legendary talk on doing great work. Applies to any field. Read it once a year.",
    type: "post",
    importance: 3,
    category: "culture",
    tags: ["career", "research", "classic"],
  },
  {
    title: "AI 2027",
    href: "https://ai-2027.com/",
    annotation: "Detailed scenario planning if current scaling holds. Optimistic? Terrifying? Depends on your priors.",
    type: "post",
    importance: 2,
    category: "culture",
    tags: ["forecasting", "scenarios", "strategy"],
  },
  {
    title: "Machines of Loving Grace — Dario Amodei",
    href: "https://www.darioamodei.com/essay/machines-of-loving-grace",
    annotation: "The optimistic case for powerful AI that doesn't feel naive. Engages with risks honestly.",
    type: "post",
    importance: 3,
    category: "culture",
    tags: ["anthropic", "optimism", "vision"],
  },
  {
    title: "Anthropic x Rick Rubin — Vibe Coding Book",
    href: "https://www.thewayofcode.com/",
    annotation: "Claude writes a book with Rick Rubin. Interesting experiment in human-AI creative collaboration.",
    type: "post",
    importance: 1,
    category: "culture",
    tags: ["creative", "collaboration", "anthropic"],
  },
  {
    title: "Artificial Intelligence Trends — Mary Meeker",
    href: "https://www.bondcap.com/report/pdf/Trends_Artificial_Intelligence.pdf",
    annotation: "The legendary internet trends report, now focused on AI. Data-heavy, slide-friendly.",
    type: "post",
    importance: 2,
    category: "culture",
    tags: ["trends", "data", "industry"],
  },
  {
    title: "The Gentle Singularity — Sam Altman",
    href: "https://blog.samaltman.com/the-gentle-singularity",
    annotation: "Altman argues the singularity will be gradual, not sudden. Counterpoint to hard-takeoff scenarios.",
    type: "post",
    importance: 2,
    category: "culture",
    tags: ["forecasting", "openai", "singularity"],
  },
  {
    title: "Richard Hamming Advice — A Stroke of Genius",
    href: "https://www.mccurley.org/advice/hamming_advice.html",
    annotation: "More Hamming wisdom, distilled. On courage, taste, and working on important problems.",
    type: "post",
    importance: 2,
    category: "culture",
    tags: ["career", "advice", "classic"],
  },
  {
    title: "How I Think About My Research Process — Neel Nanda",
    href: "https://www.alignmentforum.org/s/5GT3yoYM9gRmMEKqL/p/hjMy4ZxS5ogA9cTYK",
    annotation: "Practical research methodology from one of the most productive interp researchers.",
    type: "post",
    importance: 2,
    category: "culture",
    tags: ["methodology", "neel-nanda", "process"],
  },
  {
    title: "How to Read a Paper",
    href: "https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf",
    annotation: "The classic three-pass method. If you're reading more than 2 papers a week, you need a system.",
    type: "post",
    importance: 2,
    category: "culture",
    tags: ["methodology", "reading", "academic"],
  },
  {
    title: "It Looks Like You're Trying to Take Over the World — Gwern",
    href: "https://gwern.net/fiction/clippy",
    annotation: "Gwern's fictional narrative of a misaligned AI. Chilling because it's so plausible and detailed.",
    type: "post",
    importance: 2,
    category: "culture",
    tags: ["fiction", "gwern", "alignment"],
  },
  {
    title: "Gen AI as Seniority-Based Technological Change — Harvard",
    href: "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5425555",
    annotation: "AI benefits senior employees more than juniors. Counterintuitive finding with big workforce implications.",
    type: "paper",
    importance: 1,
    category: "culture",
    tags: ["economics", "workforce", "harvard"],
  },
  {
    title: "Top AI Researchers in the World (with Papers)",
    href: "https://www.metislist.com/",
    annotation: "Ranked by citation metrics. Useful for finding whose work to follow.",
    type: "tool",
    importance: 1,
    category: "culture",
    tags: ["reference", "researchers", "rankings"],
  },
  {
    title: "Yann LeCun — Mathematical Obstacles on the Way to Human-Level AI",
    href: "https://www.youtube.com/watch?v=ETZfkkv6V7Y",
    annotation: "LeCun's contrarian view on why autoregressive models won't reach AGI. JEPA proposal.",
    type: "video",
    importance: 2,
    category: "culture",
    tags: ["lecture", "lecun", "architecture"],
  },
]

// ─── Tools & Platforms ─────────────────────────

const tools: LibraryResource[] = [
  {
    title: "Anthropic's Open-Sourced Circuit Tracing Tools",
    href: "https://www.anthropic.com/research/open-source-circuit-tracing",
    annotation: "The actual tools behind Anthropic's circuit papers. Your scalpel for model dissection.",
    type: "tool",
    importance: 2,
    category: "tools",
    tags: ["anthropic", "circuits", "open-source"],
  },
  {
    title: "Research Rabbit",
    href: "https://www.researchrabbit.ai/",
    annotation: "Spotify Discover Weekly for academic papers. Essential for navigating the arXiv firehose.",
    type: "tool",
    importance: 2,
    category: "tools",
    tags: ["discovery", "papers", "productivity"],
  },
  {
    title: "AlphaXiv",
    href: "https://www.alphaxiv.org/",
    annotation: "Inline comments on arXiv papers. Discussions often more illuminating than the paper.",
    type: "tool",
    importance: 2,
    category: "tools",
    tags: ["discussion", "papers", "community"],
  },
  {
    title: "Moonlight: AI Research Paper Reader",
    href: "https://chromewebstore.google.com/detail/moonlight-ai-colleague-fo/lhipdkibljepmfojllcfflfflhflcbgi",
    annotation: "AI-powered paper reader that understands math. Highlights contributions, explains notation.",
    type: "tool",
    importance: 2,
    category: "tools",
    tags: ["reading", "AI-powered", "papers"],
  },
  {
    title: "AI Scientist — First Publication",
    href: "https://sakana.ai/ai-scientist-first-publication/",
    annotation: "Sakana's AI that writes, reviews, and submits papers autonomously. The future or the hype?",
    type: "tool",
    importance: 1,
    category: "tools",
    tags: ["automation", "research", "sakana"],
  },
  {
    title: "ML Collective",
    href: "https://mlcollective.org/community/",
    annotation: "Open research community pairing indie researchers with mentors. Your on-ramp to ML research.",
    type: "community",
    importance: 2,
    category: "tools",
    tags: ["community", "mentorship", "research"],
  },
  {
    title: "Career Roadmaps",
    href: "https://roadmap.sh/",
    annotation: "Visual learning paths for AI/ML engineering. Good for identifying skill gaps.",
    type: "tool",
    importance: 1,
    category: "tools",
    tags: ["career", "learning", "roadmap"],
  },
  {
    title: "Stanford CS 25 — Transformers United",
    href: "https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM",
    annotation: "Free seminar series with different transformer researcher each week. Worth watching at 2x.",
    type: "course",
    importance: 2,
    category: "tools",
    tags: ["course", "stanford", "transformers"],
  },
]

// ─── All Resources ─────────────────────────────

export const allResources: LibraryResource[] = [
  ...interpretability,
  ...safety,
  ...agents,
  ...foundations,
  ...infrastructure,
  ...culture,
  ...tools,
]

// ─── Helpers ───────────────────────────────────

export function getResourcesByCategory(category: ResourceCategory): LibraryResource[] {
  return allResources.filter((r) => r.category === category)
}

export function getResourcesByType(type: ResourceType): LibraryResource[] {
  return allResources.filter((r) => r.type === type)
}

export function getAllCategories(): ResourceCategory[] {
  return ["interpretability", "agents", "safety", "foundations", "infrastructure", "culture", "tools"]
}

export function getAllTypes(): ResourceType[] {
  return ["paper", "post", "video", "tool", "course", "book", "community"]
}

export function getCategoryLabel(category: ResourceCategory): string {
  const labels: Record<ResourceCategory, string> = {
    interpretability: "Interpretability",
    agents: "Agents",
    safety: "Safety & Alignment",
    foundations: "Foundations",
    infrastructure: "Infrastructure",
    culture: "Culture & Perspectives",
    tools: "Tools & Platforms",
  }
  return labels[category]
}

export function getTypeLabel(type: ResourceType): string {
  const labels: Record<ResourceType, string> = {
    paper: "Paper",
    post: "Post",
    video: "Video",
    tool: "Tool",
    course: "Course",
    book: "Book",
    community: "Community",
  }
  return labels[type]
}
